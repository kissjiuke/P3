def all():
    import base64
    import inspect
    import streamlit as st
    import pandas as pd
    import matplotlib.pyplot as plt
    import numpy as np
    import os
    import cv2
    import sweetviz as sv
    import xgboost as xgb
    from xgboost import XGBRegressor
    from xgboost import DMatrix
    from sklearn.model_selection import train_test_split
    from sklearn.ensemble import RandomForestRegressor
    from sklearn.linear_model import LinearRegression, LogisticRegression
    from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
    from sklearn.tree import DecisionTreeRegressor
    from xgboost import XGBRegressor
    from streamlit.components.v1 import html
    import sys
    import json
    import sweetviz as sv
    import seaborn as sns
    from pyecharts.charts import Bar
    from pyecharts.charts import Scatter  # å¯¼å…¥æ•£ç‚¹å›¾æ¨¡å—
    from pyecharts.charts import Pie  # å¯¼å…¥é¥¼å›¾æ¨¡å—
    from pyecharts.globals import ChartType, SymbolType
    from pyecharts import options as opts  # å¯¼å…¥é…ç½®é¡¹
    from pyecharts.globals import ThemeType  # ä¸»é¢˜é…ç½®é¡¹
    from pyecharts.globals import JsCode  # å¯ä»¥ç”¨äºæ‰§è¡ŒJsä»£ç 
    import os
    from streamlit.components.v1 import html
    from streamlit_option_menu import option_menu
    from pyecharts.charts import Line
    from pyecharts.charts import Tab
    from pyecharts.charts import Bar
    from pyecharts.globals import ThemeType
    from pyecharts import options as opts
    from pyecharts.globals import CurrentConfig
    import matplotx
    from geopy.distance import geodesic
    from folium import plugins
    from folium.plugins import AntPath
    from sklearn.ensemble import RandomForestRegressor
    from sklearn.model_selection import train_test_split
    import time
    import torch
    from torch.utils.data import TensorDataset,DataLoader,random_split
    from torchvision import transforms
    import torch.nn.functional as F
    #import adaxgb
    import glob
    from sklearn.metrics import confusion_matrix
    import seaborn as sns
    from torchvision import datasets, transforms
    from torch.utils.data import DataLoader, random_split
    import torch
    import torch.nn as nn
    import torch.optim as optim
    import torchvision.transforms as transforms
    from torchvision.datasets import ImageFolder
    from PIL import ImageFile
    from PIL import Image
    from rich import print
    # å¿½ç•¥æŸåçš„å›¾ç‰‡
    ImageFile.LOAD_TRUNCATED_IMAGES = True

    with st.sidebar:
        selected = option_menu(
            menu_title='åƒåœ¾åˆ†ç±»',
            options=['Home', 'æ•°æ®å±•ç¤º','æ¨¡å‹æ­å»º', 'å›¾åƒè¯†åˆ«', 'æ„è§åé¦ˆ'],  # æ¯ä¸ªé€‰é¡¹åç§°
            icons=['house', 'graph-up', 'graph-up-arrow'],  # æ¯ä¸ªé€‰é¡¹å›¾æ ‡
            menu_icon='cast',  # æ ‡é¢˜æ—è¾¹çš„å›¾æ ‡
            default_index=0,  # é»˜è®¤é€‰é¡¹
            orientation='vertical',  # horizontalæ°´å¹³
            styles={
                "container": {"padding": "5!important",
                              "background-color": "#d3e3fd"},  # è°ƒæ•´èœå•çš„Divå®¹å™¨
                "icon": {"color": "orange",
                         "font-size": "22px"},  # è°ƒæ•´å›¾æ ‡æ ·å¼
                "nav-link": {"font-size": "16px",
                             "font-weight": "bold",
                             "background-color": "#f7f8fd",  # é€‰é¡¹çš„èƒŒæ™¯é¢œè‰²
                             "color": 'black'  # é€‰é¡¹çš„å­—ä½“é¢œè‰²
                             },  # æ¯ä¸ªé€‰é¡¹æ–‡æœ¬çš„æ ·å¼
                "nav-link-selected": {"background-color": "#89e9f3",  # é€‰ä¸­é€‰é¡¹çš„èƒŒæ™¯é¢œè‰²
                                      "color": "red"  # é€‰ä¸­é€‰é¡¹çš„å­—ä½“é¢œè‰²
                                      }  # é€‰ä¸­é€‰é¡¹çš„æ ·å¼
            },
        )
    with open('./data/infos.txt', 'r', encoding='utf-8') as f:
        infos = f.read()

    if selected == 'Home':
        # ä½¿ç”¨ st.markdown æ¸²æŸ“ HTML æ ¼å¼çš„å†…å®¹
        st.markdown('# <div style="text-align: center; color: #0276fe;">åŸºäºæ·±åº¦å­¦ä¹ çš„åƒåœ¾åˆ†ç±»å›¾åƒè¯†åˆ«ç³»ç»Ÿ</div>', unsafe_allow_html=True)
        # æ¸²æŸ“æ–‡ä»¶ä¸­çš„å†…å®¹ï¼Œç¡®ä¿æ”¯æŒ HTML æ ‡ç­¾
        st.markdown(f'## <div style="text-align: center; color: #3ee076;">{infos}</div>', unsafe_allow_html=True)

    if selected == 'Home':
        with st.sidebar:
            selected_sub = option_menu(
                key='cat',
                menu_title=None,
                options=['ç­çº§ï¼šäººå·¥æ™ºèƒ½1ç­', 'å°ç»„ï¼š7ç»„',
                         f'ç»„é•¿ï¼šéƒ­é‡‘å¹³'],
                icons=['house', 'clipboard2', 'airplane'],
                menu_icon='browser-firefox',
                default_index=0,
                orientation='vertical',
                styles={
                    "container": {"padding": "5!important",
                                  "background-color": "#ffffff"},
                    "icon": {"color": "orange", "font-size": "16px"},
                    "nav-link": {"font-size": "16px", "font-weight": "bold",
                                 "background-color": "#f7f8fd",
                                 "color": 'black'
                                 },
                    "nav-link-selected": {"background-color": "#89e9f3",
                                          "color": "orange"
                                          }
                }
            )
    if selected == 'æ•°æ®å±•ç¤º':
        st.title('éƒ¨åˆ†æ•°æ®é›†é¢„è§ˆ')
        tab1,tab2,tab3,tab4 = st.tabs(['Harmful', 'Kitchen', 'Recyclable', 'Other'])
        with tab1:
            image_paths = [
                './picture/harmful/h1.jpg',
                './picture/harmful/h2.jpg',
                './picture/harmful/h3.jpg',
                './picture/harmful/h4.jpg',
                './picture/harmful/h5.jpg',
                './picture/harmful/h6.jpg',
                './picture/harmful/h7.jpg',
                './picture/harmful/h8.jpg',
                './picture/harmful/h9.jpg',
                './picture/harmful/h10.jpg',
                './picture/harmful/h11.jpg',
                './picture/harmful/h12.jpg',
            ]
            columns_per_row=4
            for i in range(0, len(image_paths), columns_per_row):
                cols = st.columns(columns_per_row)
                for j, col in enumerate(cols):
                    if i + j < len(image_paths):
                        col.image(image_paths[i + j], caption=f"Image {i + j + 1}", )
        with tab2:
            image_paths = [
                './picture/kitchen/k1.jpg',
                './picture/kitchen/k2.jpg',
                './picture/kitchen/k3.jpg',
                './picture/kitchen/k4.jpg',
                './picture/kitchen/k5.jpg',
                './picture/kitchen/k6.jpg',
                './picture/kitchen/k7.jpg',
                './picture/kitchen/k8.jpg',
                './picture/kitchen/k9.jpg',
                './picture/kitchen/k10.jpg',
                './picture/kitchen/k11.jpg',
                './picture/kitchen/k12.jpg',
            ]
            columns_per_row = 4
            for i in range(0, len(image_paths), columns_per_row):
                cols = st.columns(columns_per_row)
                for j, col in enumerate(cols):
                    if i + j < len(image_paths):
                        col.image(image_paths[i + j], caption=f"Image {i + j + 1}", )
        with tab3:
            image_paths = [
                './picture/recyclable/r1.jpg',
                './picture/recyclable/r2.jpg',
                './picture/recyclable/r3.jpg',
                './picture/recyclable/r4.jpg',
                './picture/recyclable/r5.jpg',
                './picture/recyclable/r6.jpg',
                './picture/recyclable/r7.jpg',
                './picture/recyclable/r8.jpg',
                './picture/recyclable/r9.jpg',
                './picture/recyclable/r10.jpg',
                './picture/recyclable/r11.jpg',
                './picture/recyclable/r12.jpg',
            ]
            columns_per_row = 4
            for i in range(0, len(image_paths), columns_per_row):
                cols = st.columns(columns_per_row)
                for j, col in enumerate(cols):
                    if i + j < len(image_paths):
                        col.image(image_paths[i + j], caption=f"Image {i + j + 1}", )
        with tab4:
            image_paths = [
                './picture/other/o1.jpg',
                './picture/other/o2.jpg',
                './picture/other/o3.jpg',
                './picture/other/o4.jpg',
                './picture/other/o5.jpg',
                './picture/other/o6.jpg',
                './picture/other/o7.jpg',
                './picture/other/o8.jpg',
                './picture/other/o9.jpg',
                './picture/other/o10.jpg',
                './picture/other/o11.jpg',
                './picture/other/o12.jpg',
            ]
            columns_per_row = 4
            for i in range(0, len(image_paths), columns_per_row):
                cols = st.columns(columns_per_row)
                for j, col in enumerate(cols):
                    if i + j < len(image_paths):
                        col.image(image_paths[i + j], caption=f"Image {i + j + 1}")

    if selected == 'æ¨¡å‹æ­å»º':
        with st.sidebar:
            selected_sub1 = option_menu(
                key='cat',
                menu_title=None,
                options=['VGGæ¨¡å‹', 'AlexNetæ¨¡å‹', 'ResNetæ¨¡å‹', 'LeNet5æ¨¡å‹'],
                icons=['house', 'clipboard2', 'airplane'],
                menu_icon='browser-firefox',
                default_index=0,
                orientation='vertical',
                styles={
                    "container": {"padding": "5!important",
                                  "background-color": "#ffffff"},
                    "icon": {"color": "orange", "font-size": "16px"},
                    "nav-link": {"font-size": "16px",
                                 "font-weight": "bold",
                                 "background-color": "#f7f8fd",
                                 "color": 'black'
                                 },
                    "nav-link-selected": {"background-color": "#89e9f3",
                                          "color": "orange"
                                          }
                }
            )
        if selected_sub1 == 'VGGæ¨¡å‹':
            with st.sidebar:
                selected_sub = option_menu(
                    menu_title='ä½œè€…ä¿¡æ¯',
                    options=['Authorï¼šæé›…èŠ³', 'å­¦å·ï¼š2228724106', 'ç‚¹å‡»æ‰“èµè¯¥ä½œè€…'],
                    icons=['house', 'clipboard2', 'airplane'],
                    menu_icon='browser-firefox',
                    default_index=0,
                    orientation='vertical',
                    styles={
                        "container": {"padding": "5!important",
                                      "background-color": "#ffffff"},
                        "icon": {"color": "orange", "font-size": "16px"},
                        "nav-link": {"font-size": "16px", "font-weight": "bold",
                                     "background-color": "#f7f8fd",
                                     "color": 'black'
                                     },
                        "nav-link-selected": {"background-color": "#89e9f3",
                                              "color": "orange"
                                              }
                    }
                )
            if (selected_sub == 'å­¦å·ï¼š2228724106') | (selected_sub == 'Authorï¼šæé›…èŠ³'):
                tab1, tab2, tab3, tab4, tab5 = st.tabs(['VGG11', 'VGG13', 'VGG16', 'VGG19', 'æ¨¡å‹é¢„æµ‹å›¾åƒ'])

                with tab1:
                    st.write('è®­ç»ƒè¿‡ç¨‹ä¸­ç²¾ç¡®åº¦å’ŒæŸå¤±çš„å˜åŒ–æƒ…å†µ')
                    st.image('./image/VGG11æŠ˜çº¿å›¾.png')
                    st.write('æ··æ·†çŸ©é˜µ')
                    st.image('./image/VGG11æ··æ·†çŸ©é˜µ.png')
                    st.write('åˆ†ç±»æŠ¥å‘Š')
                    st.image('./image/VGG11åˆ†ç±»æŠ¥å‘Š.png')

                with tab2:
                    st.write('è®­ç»ƒè¿‡ç¨‹ä¸­ç²¾ç¡®åº¦å’ŒæŸå¤±çš„å˜åŒ–æƒ…å†µ')
                    st.image('./image/VGG13æŠ˜çº¿å›¾.png')
                    st.write('æ··æ·†çŸ©é˜µ')
                    st.image('./image/VGG13æ··æ·†çŸ©é˜µ.png')
                    st.write('åˆ†ç±»æŠ¥å‘Š')
                    st.image('./image/VGG13åˆ†ç±»æŠ¥å‘Š.png')

                with tab3:
                    st.write('è®­ç»ƒè¿‡ç¨‹ä¸­ç²¾ç¡®åº¦å’ŒæŸå¤±çš„å˜åŒ–æƒ…å†µ')
                    st.image('./image/VGG16æŠ˜çº¿å›¾.png')
                    st.write('æ··æ·†çŸ©é˜µ')
                    st.image('./image/VGG16æ··æ·†çŸ©é˜µ.png')
                    st.write('åˆ†ç±»æŠ¥å‘Š')
                    st.image('./image/VGG16åˆ†ç±»æŠ¥å‘Š.png')

                with tab4:
                    st.write('è®­ç»ƒè¿‡ç¨‹ä¸­ç²¾ç¡®åº¦å’ŒæŸå¤±çš„å˜åŒ–æƒ…å†µ')
                    st.image('./image/VGG19æŠ˜çº¿å›¾.png')
                    st.write('æ··æ·†çŸ©é˜µ')
                    st.image('./image/VGG19æ··æ·†çŸ©é˜µ.png')
                    st.write('åˆ†ç±»æŠ¥å‘Š')
                    st.image('./image/VGG19åˆ†ç±»æŠ¥å‘Š.png')

                with tab5:
                    transform = transforms.Compose([
                        transforms.Grayscale(num_output_channels=3),
                        transforms.Resize((224, 224)),  # è°ƒæ•´å›¾åƒå¤§å°
                        transforms.ToTensor(),  # è½¬ä¸ºå¼ é‡
                        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # å½’ä¸€åŒ–
                    ])

                    cfg = {
                        'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],
                        'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],
                        'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],
                        'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512,
                                  512,
                                  512, 'M'],
                    }


                    class VGG(nn.Module):
                        def __init__(self, vgg_name, num_classes):
                            super(VGG, self).__init__()
                            self.features = self._make_layers(cfg[vgg_name])
                            self.classifier = nn.Sequential(
                                nn.Linear(512 * 7 * 7, 512),  # å‡è®¾è¾“å…¥å›¾åƒä¸º 224x224ï¼Œç»è¿‡5æ¬¡æœ€å¤§æ± åŒ–åç‰¹å¾å›¾å°ºå¯¸ä¸º 7x7
                                nn.ReLU(inplace=True),
                                nn.Dropout(0.2),
                                nn.Linear(512, 256),
                                nn.ReLU(inplace=True),
                                nn.Dropout(0.2),  # Dropoutå±‚,å¯ä»¥0.2æ¦‚ç‡ä¸¢å¼ƒä¸€äº›ç¥ç»å…ƒ
                                nn.Linear(256, num_classes)  # å¤šåˆ†ç±»ä»»åŠ¡
                            )

                        def forward(self, x):
                            out = self.features(x)  # é€šè¿‡ç‰¹å¾å±‚æå–
                            out = out.view(out.size(0), -1)
                            out = self.classifier(out)
                            out = F.log_softmax(out, dim=1)  # æ·»åŠ softmaxæ¿€æ´»å‡½æ•°
                            return out

                        # æ ¹æ®é…ç½®åˆ›å»ºç½‘ç»œå±‚
                        def _make_layers(self, cfg):
                            layers = []
                            in_channels = 3  # è¾“å…¥çš„é€šé“æ•°
                            for x in cfg:
                                if x == 'M':  # è¯´æ˜é‡åˆ°äº†æœ€å¤§æ± åŒ–å±‚
                                    layers += [nn.MaxPool2d(kernel_size=2, stride=2, padding=0)]
                                else:
                                    layers += [nn.Conv2d(in_channels, x, kernel_size=3, stride=1, padding=1),
                                               nn.BatchNorm2d(x),
                                               nn.ReLU()
                                               ]
                                    in_channels = x
                            return nn.Sequential(*layers)


                    device = 'cuda' if torch.cuda.is_available() else 'cpu'
                    VGG11 = VGG(vgg_name='VGG11', num_classes=4).to(device)
                    VGG13 = VGG(vgg_name='VGG13', num_classes=4).to(device)
                    VGG16 = VGG(vgg_name='VGG16', num_classes=4).to(device)
                    VGG19 = VGG(vgg_name='VGG19', num_classes=4).to(device)


                    def predict_image(image_path, model, classes, device):
                        # åŠ è½½å’Œé¢„å¤„ç†å›¾åƒ
                        image = Image.open(image_path).convert('RGB')
                        transform = transforms.Compose([
                            transforms.Resize((224, 224)),
                            transforms.ToTensor(),
                            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
                        ])
                        img = transform(image)
                        img = img.unsqueeze(0).to(device)

                        # è¿›è¡Œé¢„æµ‹
                        model.eval()
                        with torch.no_grad():
                            output = model(img)
                        prob = F.softmax(output, dim=1)
                        value, predicted = torch.max(output.data, 1)

                        # è¿”å›é¢„æµ‹ç»“æœ
                        return prob, predicted.item(), classes[predicted.item()]


                    # è®¾ç½®è®¾å¤‡
                    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
                    # åˆ›å»ºæ¨¡å‹å®ä¾‹å¹¶åŠ è½½æƒé‡
                    model11 = VGG11
                    model13 = VGG13
                    model16 = VGG16
                    model19 = VGG19
                    # åŠ è½½æ¨¡å‹çš„state_dict
                    state_dict11 = torch.load('./model/VGG11.pth', map_location=device)
                    model11.load_state_dict(state_dict11)
                    state_dict13 = torch.load('./model/VGG13.pth', map_location=device)
                    model13.load_state_dict(state_dict13)
                    state_dict16 = torch.load('./model/VGG16.pth', map_location=device)
                    model16.load_state_dict(state_dict16)
                    state_dict19 = torch.load('./model/VGG19.pth', map_location=device)
                    model19.load_state_dict(state_dict19)

                    # è®¾ç½®è®¾å¤‡
                    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
                    model11 = model11.to(device)
                    model11.eval()  # è°ƒæ•´ä¸ºè¯„ä¼°æ¨¡å¼
                    model13 = model13.to(device)
                    model13.eval()  # è°ƒæ•´ä¸ºè¯„ä¼°æ¨¡å¼
                    model16 = model16.to(device)
                    model16.eval()  # è°ƒæ•´ä¸ºè¯„ä¼°æ¨¡å¼
                    model19 = model19.to(device)
                    model19.eval()  # è°ƒæ•´ä¸ºè¯„ä¼°æ¨¡å¼

                    # å®šä¹‰ç±»åˆ«
                    classes = ('Harmful', 'Kitchen', 'Other', 'Recyclable')
                    # æ–‡ä»¶ä¸Šä¼ å™¨
                    file_uploader = st.file_uploader("Choose an image...", type=["jpg", "png"])
                    if file_uploader is not None:
                        # é¢„æµ‹å›¾åƒ
                        results = []
                        for model_name, model in [('VGG11', model11), ('VGG13', model13), ('VGG16', model16),
                                                  ('VGG19', model19)]:
                            prob, predicted_index, pred_class = predict_image(file_uploader, model, classes, device)
                            results.append((model_name, prob, predicted_index, pred_class))

                        # å±•ç¤ºé¢„æµ‹ç»“æœ
                        st.image(file_uploader, caption='Uploaded Image', use_container_width=True)  # å¦‚æœéœ€è¦å±•ç¤ºä¸Šä¼ çš„å›¾åƒï¼Œå¯ä»¥å–æ¶ˆæ³¨é‡Šè¿™è¡Œä»£ç 
                        for model_name, prob, predicted_index, pred_class in results:
                            st.write(f"æ¨¡å‹{model_name}çš„é¢„æµ‹:")
                            for i, p in enumerate(prob[0], start=0):  # å‡è®¾ prob[0] æ˜¯ä¸€ä¸ªåŒ…å«æ‰€æœ‰æ¦‚ç‡çš„å¼ é‡
                                st.write(f'é¢„æµ‹ä¸º{classes[i]}åƒåœ¾æ¦‚ç‡ä¸º: {p:.4f}')
                            st.write(f"æœ€ç»ˆé¢„æµ‹ç±»åˆ«ä¸º: {pred_class}")
                    # å¦‚æœæ²¡æœ‰ä¸Šä¼ æ–‡ä»¶ï¼Œåˆ™å±•ç¤ºè¯´æ˜
                    else:
                        st.write("Please upload an image to see the prediction.")
            if selected_sub == 'ç‚¹å‡»æ‰“èµè¯¥ä½œè€…':
                st.image("./picture/æ„Ÿè°¢.gif", width=500)
        if selected_sub1 == 'AlexNetæ¨¡å‹':
            with st.sidebar:
                selected_sub = option_menu(
                    menu_title='ä½œè€…ä¿¡æ¯',
                    options=['Authorï¼šé©¬æ–‡é™', 'å­¦å·ï¼š2228324096', 'ç‚¹å‡»æ‰“èµè¯¥ä½œè€…'],
                    icons=['house', 'clipboard2', 'airplane'],
                    menu_icon='browser-firefox',
                    default_index=0,
                    orientation='vertical',
                    styles={
                        "container": {"padding": "5!important",
                                      "background-color": "#ffffff"},
                        "icon": {"color": "orange", "font-size": "16px"},
                        "nav-link": {"font-size": "16px", "font-weight": "bold",
                                     "background-color": "#f7f8fd",
                                     "color": 'black'
                                     },
                        "nav-link-selected": {"background-color": "#89e9f3",
                                              "color": "orange"
                                              }
                    }
                )
            if (selected_sub == 'å­¦å·ï¼š2228324096') | (selected_sub == 'Authorï¼šé©¬æ–‡é™'):
                tab1, tab2, tab3,tab4 = st.tabs(['Tab 1', 'Tab 2', 'Tab 3','Tab 4'])
                with tab1:
                    class AlexNet(nn.Module):
                        def __init__(self):
                            super(AlexNet, self).__init__()
                            self.conv1 = nn.Sequential(
                                nn.Conv2d(in_channels=3, out_channels=64, kernel_size=11, stride=4),
                                nn.ReLU(inplace=True),
                                nn.MaxPool2d(kernel_size=3, stride=2)
                            )
                            self.conv2 = nn.Sequential(
                                nn.Conv2d(in_channels=64, out_channels=192, kernel_size=5, stride=1, padding=2),
                                nn.ReLU(inplace=True),
                                nn.MaxPool2d(kernel_size=3, stride=2)
                            )
                            self.conv3 = nn.Sequential(
                                nn.Conv2d(in_channels=192, out_channels=384, kernel_size=3, stride=1, padding=1),
                                nn.ReLU(inplace=True),
                                nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, stride=1, padding=1),
                                nn.ReLU(inplace=True),
                                nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),
                                nn.ReLU(inplace=True)
                            )
                            self.flatten = nn.Flatten()
                            self.fc = nn.Sequential(
                                nn.Linear(9216, 4096),
                                nn.ReLU(inplace=True),
                                nn.Linear(4096, 4096),
                                nn.ReLU(inplace=True),
                                nn.Linear(4096, 4)
                            )

                        def forward(self, x):
                            x = self.conv1(x)
                            x = self.conv2(x)
                            x = self.conv3(x)
                            x = self.flatten(x)
                            x = self.fc(x)
                            return x


                    device = 'cuda' if torch.cuda.is_available() else 'cpu'
                    model = AlexNet().to(device)
                    st.write('ä¸»è¦ä»£ç å±•ç¤º')
                    st.image('./image/Alexnetä¸»è¦ä»£ç å±•ç¤º.png')
                with tab2:
                    st.write('è®­ç»ƒè¿‡ç¨‹ä¸­ç²¾ç¡®åº¦å’ŒæŸå¤±çš„å˜åŒ–æƒ…å†µ')
                    st.image('./image/AlexNet.png')
                with tab3:
                    st.write('æ··æ·†çŸ©é˜µ')
                    st.image('./image/AlexNetæ··æ·†çŸ©é˜µ.png')
                with tab4:
                    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
                    # è®¾ç½®ç±»åˆ«
                    classes = {0: 'Harmful', 1: 'Kitchen', 2: 'Other', 3: 'Recyclable'}

                    # resnet18 = resnet_plus(BasicBlock, [2, 2, 2, 2]).to(device)
                    # resnet18.load_state_dict(torch.load('./model/resnet18.pth', map_location=device, weights_only=True))
                    # resnet18.eval()

                    # åˆ›å»ºæ¨¡å‹å®ä¾‹å¹¶åŠ è½½æƒé‡
                    model = AlexNet()
                    state_dict = torch.load('./model/AlexNet.pth', map_location=device)
                    model.load_state_dict(state_dict)
                    model = model.to(device)
                    model.eval()  # è°ƒæ•´ä¸ºè¯„ä¼°æ¨¡å¼


                    # å›¾åƒé¢„å¤„ç†å‡½æ•°
                    def preprocess_image(image):
                        image = cv2.resize(image, (128, 128))  # å‡è®¾è®­ç»ƒæ—¶ä½¿ç”¨224x224å¤§å°
                        transform = transforms.Compose([
                            transforms.ToTensor(),  # è½¬æ¢ä¸ºTensorå¹¶è‡ªåŠ¨å½’ä¸€åŒ–åˆ°[0, 1]
                            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # æ ‡å‡†åŒ–
                        ])
                        img = transform(image).unsqueeze(0)  # å¢åŠ æ‰¹æ¬¡ç»´åº¦
                        return img.to(device)


                    # é¢„æµ‹å‡½æ•°
                    def predict_image(image):
                        # å°†å›¾åƒå¤„ç†ä¸ºæ¨¡å‹è¾“å…¥
                        img = preprocess_image(image)
                        # æ¨¡å‹è¯„ä¼°æ¨¡å¼
                        model.eval()
                        # ä¸è·Ÿè¸ªæ¢¯åº¦ï¼Œå‡å°‘å†…å­˜å’Œè®¡ç®—é‡
                        with torch.no_grad():
                            output = model(img)
                        # åº”ç”¨softmaxè·å–æ¦‚ç‡åˆ†å¸ƒ
                        prob = F.softmax(output, dim=1)
                        # è·å–é¢„æµ‹ç±»åˆ«
                        _, predicted = torch.max(output, 1)
                        pred_class = classes[predicted.item()]
                        return prob, predicted.item(), pred_class


                    st.write("AlexNetåƒåœ¾åˆ†ç±»é¢„æµ‹")
                    uploaded_file = st.file_uploader("ä¸Šä¼ å›¾ç‰‡ (png/jpg æ ¼å¼)", type=["png", "jpg"])
                    if uploaded_file is not None:
                        # æ˜¾ç¤ºä¸Šä¼ çš„å›¾ç‰‡
                        image = Image.open(uploaded_file)
                        st.image(image, caption="ä¸Šä¼ çš„å›¾ç‰‡", use_container_width=True)
                        # å°†PILå›¾ç‰‡è½¬æ¢ä¸ºOpenCVæ ¼å¼è¿›è¡Œå¤„ç†
                        image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
                        # è®°å½•é¢„æµ‹å¼€å§‹çš„æ—¶é—´
                        start_time = time.time()
                        prob, class_idx, class_name = predict_image(image)
                        # è®°å½•é¢„æµ‹ç»“æŸçš„æ—¶é—´å¹¶è®¡ç®—é¢„æµ‹æ—¶é—´
                        end_time = time.time()
                        prediction_time = end_time - start_time
                        # æ˜¾ç¤ºé¢„æµ‹ç»“æœ
                        st.write(f"é¢„æµ‹ç±»åˆ«: {class_name}")
                        st.write(f"é¢„æµ‹æ—¶é—´: {prediction_time:.4f} ç§’")  # æ˜¾ç¤ºé¢„æµ‹æ—¶é—´
            if selected_sub == 'ç‚¹å‡»æ‰“èµè¯¥ä½œè€…':
                st.image("./picture/æ„Ÿè°¢.gif", width=500)
        if selected_sub1 == 'ResNetæ¨¡å‹':
            with st.sidebar:
                selected_sub = option_menu(
                    menu_title='ä½œè€…ä¿¡æ¯',
                    options=['Authorï¼šéŸ©æ¢¦é˜', 'å­¦å·ï¼š2228724025', 'ç‚¹å‡»æ‰“èµè¯¥ä½œè€…'],
                    icons=['house', 'clipboard2', 'airplane'],
                    menu_icon='browser-firefox',
                    default_index=0,
                    orientation='vertical',
                    styles={
                        "container": {"padding": "5!important",
                                      "background-color": "#ffffff"},
                        "icon": {"color": "orange", "font-size": "16px"},
                        "nav-link": {"font-size": "16px", "font-weight": "bold",
                                     "background-color": "#f7f8fd",
                                     "color": 'black'
                                     },
                        "nav-link-selected": {"background-color": "#89e9f3",
                                              "color": "orange"
                                              }
                    }
                )
            if (selected_sub == 'å­¦å·ï¼š2228724025') | (selected_sub == 'Authorï¼šéŸ©æ¢¦é˜'):
                tab1, tab2, tab3 = st.tabs(['Tab 1', 'Tab 2', 'Tab 3'])
                with tab1:
                    st.write('è®­ç»ƒè¿‡ç¨‹ä¸­ç²¾ç¡®åº¦å’ŒæŸå¤±çš„å˜åŒ–æƒ…å†µ')
                    st.image('./image/resnet18.png')
                with tab2:
                    st.write('æ··æ·†çŸ©é˜µ')
                    st.image('./image/resnet18æ··æ·†çŸ©é˜µ.png')
                with tab3:
                    class BasicBlock(nn.Module):
                        expansion = 1  #

                        def __init__(self, input_channels, output_channels, strides=1):
                            super(BasicBlock, self).__init__()  # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–æ–¹æ³•
                            # ç¬¬ä¸€ä¸ªå·åŸºå±‚,è¾“å…¥ä¸ºinput_channels,è¾“å‡ºä¸ºnum_channels
                            self.conv1 = nn.Conv2d(input_channels, output_channels, kernel_size=3,
                                                   padding=1, stride=strides, bias=False)
                            self.bn1 = nn.BatchNorm2d(output_channels)
                            self.relu = nn.ReLU(inplace=True)  # å…è®¸åŸåœ°ä¿®æ”¹,å¯ä»¥å‡å°‘å†…å­˜çš„ä½¿ç”¨

                            # ç¬¬ä¸€ä¸ªå·åŸºå±‚,è¾“å…¥ä¸ºnum_channels,è¾“å‡ºä¸ºnum_channels
                            self.conv2 = nn.Conv2d(output_channels, output_channels, kernel_size=3, padding=1,
                                                   stride=1, bias=False
                                                   )
                            # å®šä¹‰ä¸¤ä¸ªæ‰¹é‡å½’ä¸€åŒ–å±‚,
                            self.bn2 = nn.BatchNorm2d(output_channels)
                            # åˆ›å»ºä¸€ä¸ªå¿«æ·è¿æ¥,å¦‚æœè¾“å…¥è¾“å‡ºä¸ä¸€è‡´,ä½¿ç”¨1x1å·ç§¯æ”¹å˜è¾“å‡º
                            self.shortcut = nn.Sequential()
                            if strides != 1 or input_channels != self.expansion * output_channels:
                                self.shortcut = nn.Sequential(
                                    nn.Conv2d(input_channels, self.expansion * output_channels,
                                              kernel_size=1, stride=strides, bias=False),
                                    nn.BatchNorm2d(self.expansion * output_channels)
                                )

                        def forward(self, x):
                            output = self.relu(self.bn1(self.conv1(x)))  # ç½‘ç»œçš„ç¬¬ä¸€å±‚
                            output = self.bn2(self.conv2(output))  # ç½‘ç»œlç¬¬äºŒå±‚
                            output += self.shortcut(x)  # å°†å¿«æ·è¿æ¥çš„è¾“å‡º,åŠ åˆ°ç¬¬äºŒä¸ªå·ç§¯çš„è¾“å‡ºä¸Š
                            output = self.relu(output)
                            return output

                    # åŸºæœ¬ç½‘æ ¼æ­å»º
                    class ResNet_plus(nn.Module):
                        def __init__(self, block, num_block, num_class=10, init_channels=3):
                            super(ResNet_plus, self).__init__()
                            self.input_channels = 64
                            self.features = nn.Sequential(
                                nn.Conv2d(init_channels, 64, kernel_size=3, stride=1, padding=1, bias=False),
                                nn.BatchNorm2d(64),
                                nn.ReLU(inplace=True)
                            )
                            # æ„å»ºæ®‹å·®å±‚ï¼Œæ¯ä¸ªå±‚æœ‰å¤šä¸ªæ®‹å·®å—æ„å»º
                            self.layer1 = self._make_layer(block, 64, num_block[0], stride=2)
                            self.layer2 = self._make_layer(block, 128, num_block[1], stride=3)
                            self.layer3 = self._make_layer(block, 256, num_block[2], stride=4)
                            self.layer4 = self._make_layer(block, 512, num_block[3], stride=5)
                            self.avgpool = nn.AvgPool2d(kernel_size=2)
                            # å…¨è¿æ¥å±‚ï¼Œè¾“å‡º10ä¸ªæ¦‚ç‡åˆ†å¸ƒ
                            self.fc = nn.Linear(512 * block.expansion, num_class)

                        def _make_layer(self, block, output_channels, num_block, stride):
                            # æ„å»ºæ®‹å·®å±‚,åŒ…å«å¤šä¸ªæ®‹å·®å—
                            strides_ = [stride] + [1] * (num_block - 1)
                            layers = []
                            for stride in strides_:
                                layers.append(block(self.input_channels, output_channels, stride))
                                self.input_channels = output_channels * block.expansion
                            return nn.Sequential(*layers)

                        def forward(self, x):
                            out = self.features(x)
                            out = self.layer1(out)
                            out = self.layer2(out)
                            out = self.layer3(out)
                            out = self.layer4(out)
                            out = self.avgpool(out)
                            out = out.view(out.size(0), -1)  # å±•å¼€åˆ°ä¸€ç»´
                            out = self.fc(out)
                            return out

                    def resnet_plus(block=BasicBlock, layers=[2, 2, 2, 2]):
                        return ResNet_plus(block, num_block=layers, num_class=4, init_channels=3)

                    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
                    # è®¾ç½®ç±»åˆ«
                    classes = {0: 'Harmful', 1: 'Kitchen', 2: 'Other', 3: 'Recyclable'}

                    # resnet18 = resnet_plus(BasicBlock, [2, 2, 2, 2]).to(device)
                    # resnet18.load_state_dict(torch.load('./model/resnet18.pth', map_location=device, weights_only=True))
                    # resnet18.eval()

                    # åˆ›å»ºæ¨¡å‹å®ä¾‹å¹¶åŠ è½½æƒé‡
                    model = resnet_plus()
                    state_dict = torch.load('./model/res2.pth', map_location=device)
                    model.load_state_dict(state_dict)
                    model = model.to(device)
                    model.eval()  # è°ƒæ•´ä¸ºè¯„ä¼°æ¨¡å¼

                    # å›¾åƒé¢„å¤„ç†å‡½æ•°
                    def preprocess_image(image):
                        image = cv2.resize(image, (128, 128))  # å‡è®¾è®­ç»ƒæ—¶ä½¿ç”¨224x224å¤§å°
                        transform = transforms.Compose([
                            transforms.ToTensor(),  # è½¬æ¢ä¸ºTensorå¹¶è‡ªåŠ¨å½’ä¸€åŒ–åˆ°[0, 1]
                            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # æ ‡å‡†åŒ–
                        ])
                        img = transform(image).unsqueeze(0)  # å¢åŠ æ‰¹æ¬¡ç»´åº¦
                        return img.to(device)

                    # é¢„æµ‹å‡½æ•°
                    def predict_image(image):
                        # å°†å›¾åƒå¤„ç†ä¸ºæ¨¡å‹è¾“å…¥
                        img = preprocess_image(image)
                        # æ¨¡å‹è¯„ä¼°æ¨¡å¼
                        model.eval()
                        # ä¸è·Ÿè¸ªæ¢¯åº¦ï¼Œå‡å°‘å†…å­˜å’Œè®¡ç®—é‡
                        with torch.no_grad():
                            output = model(img)
                        # åº”ç”¨softmaxè·å–æ¦‚ç‡åˆ†å¸ƒ
                        prob = F.softmax(output, dim=1)
                        # è·å–é¢„æµ‹ç±»åˆ«
                        _, predicted = torch.max(output, 1)
                        pred_class = classes[predicted.item()]
                        return prob, predicted.item(), pred_class

                    st.write("ResNet18åƒåœ¾åˆ†ç±»é¢„æµ‹")
                    uploaded_file = st.file_uploader("ä¸Šä¼ å›¾ç‰‡ (png/jpg æ ¼å¼)", type=["png", "jpg"])
                    if uploaded_file is not None:
                        # æ˜¾ç¤ºä¸Šä¼ çš„å›¾ç‰‡
                        image = Image.open(uploaded_file)
                        st.image(image, caption="ä¸Šä¼ çš„å›¾ç‰‡", use_column_width=True)
                        # å°†PILå›¾ç‰‡è½¬æ¢ä¸ºOpenCVæ ¼å¼è¿›è¡Œå¤„ç†
                        image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
                        # è®°å½•é¢„æµ‹å¼€å§‹çš„æ—¶é—´
                        start_time = time.time()
                        prob, class_idx, class_name = predict_image(image)
                        # è®°å½•é¢„æµ‹ç»“æŸçš„æ—¶é—´å¹¶è®¡ç®—é¢„æµ‹æ—¶é—´
                        end_time = time.time()
                        prediction_time = end_time - start_time
                        # æ˜¾ç¤ºé¢„æµ‹ç»“æœ
                        st.write(f"é¢„æµ‹ç±»åˆ«: {class_name}")
                        st.write(f"é¢„æµ‹æ—¶é—´: {prediction_time:.4f} ç§’")  # æ˜¾ç¤ºé¢„æµ‹æ—¶é—´

            if selected_sub == 'ç‚¹å‡»æ‰“èµè¯¥ä½œè€…':
                st.image("./picture/æ„Ÿè°¢.gif", width=500)
        if selected_sub1 == 'LeNet5æ¨¡å‹':
            with st.sidebar:
                selected_sub = option_menu(
                    menu_title='ä½œè€…ä¿¡æ¯',
                    options=['Authorï¼šéƒ­é‡‘å¹³', 'å­¦å·ï¼š2228724238', 'ç‚¹å‡»æ‰“èµè¯¥ä½œè€…'],
                    icons=['house', 'clipboard2', 'airplane'],
                    menu_icon='browser-firefox',
                    default_index=0,
                    orientation='vertical',
                    styles={
                        "container": {"padding": "5!important",
                                      "background-color": "#ffffff"},
                        "icon": {"color": "orange", "font-size": "16px"},
                        "nav-link": {"font-size": "16px", "font-weight": "bold",
                                     "background-color": "#f7f8fd",
                                     "color": 'black'
                                     },
                        "nav-link-selected": {"background-color": "#89e9f3",
                                              "color": "orange"
                                              }
                    }
                )
            if (selected_sub == 'å­¦å·ï¼š2228724238') | (selected_sub == 'Authorï¼šéƒ­é‡‘å¹³'):
                tab1, tab2, tab3,tab4 = st.tabs(['Tab 1', 'Tab 2', 'Tab 3','Tab 4'])
                with tab1:
                    st.write('ä¸»è¦ä»£ç å±•ç¤º')
                    st.image('./image/lenet5ç½‘ç»œæ­å»º.png')
                    class LeNet5(nn.Module):
                        def __init__(self, num_class=4):
                            super(LeNet5, self).__init__()
                            self.features = nn.Sequential(
                                nn.Conv2d(3, 6, kernel_size=5),
                                nn.ReLU(),
                                nn.AvgPool2d(kernel_size=2, stride=2),
                                nn.Conv2d(6, 16, kernel_size=5),
                                nn.ReLU(),
                                nn.AvgPool2d(kernel_size=2, stride=2)
                            )
                            self.flatten = nn.Flatten()
                            self.classifier = nn.Sequential(
                                nn.Linear(13456, 120),
                                nn.ReLU(),
                                nn.Linear(120, 84),
                                nn.ReLU(),
                                nn.Linear(84, num_class),
                            )

                        def forward(self, x):
                            x = self.features(x)
                            x = self.flatten(x)
                            x = self.classifier(x)
                            return x

                with tab2:
                    st.write('è®­ç»ƒè¿‡ç¨‹ä¸­ç²¾ç¡®åº¦å’ŒæŸå¤±çš„å˜åŒ–æƒ…å†µ')
                    st.image('./image/lenet5.png')
                with tab3:
                    st.write('æ··æ·†çŸ©é˜µ')
                    st.image('./image/lenet5æ··æ·†çŸ©é˜µ.png')

                with tab4:
                    st.write('æ¨¡å‹é¢„æµ‹å›¾åƒ')
                    # å°è£…æˆå‡½æ•°
                    def predict_image(image_path, model, classes, device):
                        # åŠ è½½å’Œé¢„å¤„ç†å›¾åƒ
                        image = Image.open(image_path).convert('RGB')
                        transform = transforms.Compose([
                            transforms.Resize((128, 128)),
                            transforms.ToTensor(),
                            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
                        ])
                        img = transform(image)
                        img = img.unsqueeze(0).to(device)

                        # è¿›è¡Œé¢„æµ‹
                        model.eval()
                        with torch.no_grad():
                            output = model(img)
                        prob = F.softmax(output, dim=1)
                        value, predicted = torch.max(output.data, 1)

                        # è¿”å›é¢„æµ‹ç»“æœ
                        return prob, predicted.item(), classes[predicted.item()]
                    # è®¾ç½®è®¾å¤‡
                    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

                    # åˆ›å»ºæ¨¡å‹å®ä¾‹å¹¶åŠ è½½æƒé‡
                    model = LeNet5()
                    state_dict = torch.load('./model/LeNet5-1.pth', map_location=device)
                    model.load_state_dict(state_dict)
                    model = model.to(device)
                    model.eval()  # è°ƒæ•´ä¸ºè¯„ä¼°æ¨¡å¼

                    # å®šä¹‰ç±»åˆ«
                    classes = ('Harmful', 'Kitchen', 'Other', 'Recyclable')

                    # æ–‡ä»¶ä¸Šä¼ å™¨
                    file_uploader = st.file_uploader("Choose an image...", type=["jpg", "png"])
                    if file_uploader is not None:
                        # é¢„æµ‹å›¾åƒ
                        prob, predicted_index, pred_class = predict_image(file_uploader, model, classes, device)

                        # å±•ç¤ºé¢„æµ‹ç»“æœ
                        st.image(file_uploader, caption='Uploaded Image', use_container_width=True)
                        for i, p in enumerate(prob[0], start=0):  # å‡è®¾ prob[0] æ˜¯ä¸€ä¸ªåŒ…å«æ‰€æœ‰æ¦‚ç‡çš„å¼ é‡
                            st.write(f'é¢„æµ‹ä¸º{classes[i]}åƒåœ¾æ¦‚ç‡ä¸º: {p:.4f}')
                        st.write(f'æœ€ç»ˆé¢„æµ‹çš„ç±»åˆ«ç´¢å¼•ä¸º: {predicted_index}')
                        st.write(f'æœ€ç»ˆé¢„æµ‹çš„ç±»åˆ«ä¸º: {pred_class}')

                    # å¦‚æœæ²¡æœ‰ä¸Šä¼ æ–‡ä»¶ï¼Œåˆ™å±•ç¤ºè¯´æ˜
                    else:
                        st.write("Please upload an image to see the prediction.")

            if selected_sub == 'ç‚¹å‡»æ‰“èµè¯¥ä½œè€…':
                st.image("./picture/æ„Ÿè°¢.gif", width=500)
    if selected == 'å›¾åƒè¯†åˆ«':
        with st.sidebar:
            selected_sub1 = option_menu(
                key='cat',
                menu_title=None,
                options=['åƒåœ¾åˆ†ç±»', ],
                icons=['house', 'clipboard2', 'airplane'],
                menu_icon='browser-firefox',
                default_index=0,
                orientation='vertical',
                styles={
                    "container": {"padding": "5!important",
                                  "background-color": "#ffffff"},
                    "icon": {"color": "orange", "font-size": "16px"},
                    "nav-link": {"font-size": "16px",
                                 "font-weight": "bold",
                                 "background-color": "#f7f8fd",
                                 "color": 'black'
                                 },
                    "nav-link-selected": {"background-color": "#89e9f3",
                                          "color": "orange"
                                          }
                }
            )
        if selected_sub1 == 'åƒåœ¾åˆ†ç±»':
            with st.sidebar:
                selected_sub = option_menu(
                    menu_title='ä½œè€…ä¿¡æ¯',
                    options=['Authorï¼šææ³“è‹', 'å­¦å·ï¼š2228724177', 'ç‚¹å‡»æ‰“èµè¯¥ä½œè€…'],
                    icons=['house', 'clipboard2', 'airplane'],
                    menu_icon='browser-firefox',
                    default_index=0,
                    orientation='vertical',
                    styles={
                        "container": {"padding": "5!important",
                                      "background-color": "#ffffff"},
                        "icon": {"color": "orange", "font-size": "16px"},
                        "nav-link": {"font-size": "16px", "font-weight": "bold",
                                     "background-color": "#f7f8fd",
                                     "color": 'black'
                                     },
                        "nav-link-selected": {"background-color": "#89e9f3",
                                              "color": "orange"
                                              }
                    }
                )
            if selected_sub == 'ç‚¹å‡»æ‰“èµè¯¥ä½œè€…':
                st.image("./picture/æ„Ÿè°¢.gif", width=500)
            if (selected_sub == 'Authorï¼šææ³“è‹') | (selected_sub == 'å­¦å·ï¼š2228724177'):
                tab1, tab2 = st.tabs(['ğŸŒAlexNet', 'ğŸ“ŠVGG'])
                with tab1:
                    class AlexNet(nn.Module):
                        def __init__(self):
                            super(AlexNet, self).__init__()
                            self.conv1 = nn.Sequential(
                                nn.Conv2d(in_channels=3, out_channels=64, kernel_size=11, stride=4),
                                nn.ReLU(inplace=True),
                                nn.MaxPool2d(kernel_size=3, stride=2)
                            )
                            self.conv2 = nn.Sequential(
                                nn.Conv2d(in_channels=64, out_channels=192, kernel_size=5, stride=1, padding=2),
                                nn.ReLU(inplace=True),
                                nn.MaxPool2d(kernel_size=3, stride=2)
                            )
                            self.conv3 = nn.Sequential(
                                nn.Conv2d(in_channels=192, out_channels=384, kernel_size=3, stride=1, padding=1),
                                nn.ReLU(inplace=True),
                                nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, stride=1, padding=1),
                                nn.ReLU(inplace=True),
                                nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),
                                nn.ReLU(inplace=True)
                            )
                            self.flatten = nn.Flatten()
                            self.fc = nn.Sequential(
                                nn.Linear(9216, 4096),
                                nn.ReLU(inplace=True),
                                nn.Linear(4096, 4096),
                                nn.ReLU(inplace=True),
                                nn.Linear(4096, 4)
                            )

                        def forward(self, x):
                            x = self.conv1(x)
                            x = self.conv2(x)
                            x = self.conv3(x)
                            x = self.flatten(x)
                            x = self.fc(x)
                            return x


                    # è®¾ç½®ç±»åˆ«
                    classes = {0: 'Harmful', 1: 'Kitchen', 2: 'Other', 3: 'Recyclable'}
                    # åˆ›å»ºæ¨¡å‹å®ä¾‹å¹¶åŠ è½½æƒé‡
                    model = AlexNet()
                    device = 'cuda' if torch.cuda.is_available() else 'cpu'
                    state_dict = torch.load('./model/AlexNet.pth', map_location=device)
                    model.load_state_dict(state_dict)
                    model = model.to(device)
                    model.eval()  # è°ƒæ•´ä¸ºè¯„ä¼°æ¨¡å¼


                    # å›¾åƒé¢„å¤„ç†å‡½æ•°
                    def preprocess_image(image):
                        image = cv2.resize(image, (128, 128))  # å‡è®¾è®­ç»ƒæ—¶ä½¿ç”¨224x224å¤§å°
                        transform = transforms.Compose([
                            transforms.ToTensor(),  # è½¬æ¢ä¸ºTensorå¹¶è‡ªåŠ¨å½’ä¸€åŒ–åˆ°[0, 1]
                            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # æ ‡å‡†åŒ–
                        ])
                        img = transform(image).unsqueeze(0)  # å¢åŠ æ‰¹æ¬¡ç»´åº¦
                        return img.to(device)


                    # é¢„æµ‹å‡½æ•°
                    def predict_image(image):
                        # å°†å›¾åƒå¤„ç†ä¸ºæ¨¡å‹è¾“å…¥
                        img = preprocess_image(image)
                        # æ¨¡å‹è¯„ä¼°æ¨¡å¼
                        model.eval()
                        # ä¸è·Ÿè¸ªæ¢¯åº¦ï¼Œå‡å°‘å†…å­˜å’Œè®¡ç®—é‡
                        with torch.no_grad():
                            output = model(img)
                        # åº”ç”¨softmaxè·å–æ¦‚ç‡åˆ†å¸ƒ
                        prob = F.softmax(output, dim=1)
                        # è·å–é¢„æµ‹ç±»åˆ«
                        _, predicted = torch.max(output, 1)
                        pred_class = classes[predicted.item()]
                        return prob, predicted.item(), pred_class



                    uploaded_file = st.file_uploader("ä¸Šä¼ å›¾ç‰‡ (png/jpg æ ¼å¼)", type=["png", "jpg"])
                    if uploaded_file is not None:
                        # æ˜¾ç¤ºä¸Šä¼ çš„å›¾ç‰‡
                        image = Image.open(uploaded_file)
                        st.image(image, caption="ä¸Šä¼ çš„å›¾ç‰‡", use_container_width=True)
                        # å°†PILå›¾ç‰‡è½¬æ¢ä¸ºOpenCVæ ¼å¼è¿›è¡Œå¤„ç†
                        image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
                        # è®°å½•é¢„æµ‹å¼€å§‹çš„æ—¶é—´
                        start_time = time.time()
                        prob, class_idx, class_name = predict_image(image)
                        # è®°å½•é¢„æµ‹ç»“æŸçš„æ—¶é—´å¹¶è®¡ç®—é¢„æµ‹æ—¶é—´
                        end_time = time.time()
                        prediction_time = end_time - start_time
                        # æ˜¾ç¤ºé¢„æµ‹ç»“æœ
                        st.write(f"é¢„æµ‹ç±»åˆ«: {class_name}")
                        st.write(f"é¢„æµ‹æ—¶é—´: {prediction_time:.4f} ç§’")  # æ˜¾ç¤ºé¢„æµ‹æ—¶é—´
                with tab2:
                    import streamlit as st
                    import torch
                    from torch import nn
                    import torch.nn.functional as F
                    from torchvision import transforms
                    from PIL import Image


                    # å®šä¹‰VGGæ¨¡å‹
                    class VGG(nn.Module):
                        def __init__(self, vgg_name, num_classes):
                            super(VGG, self).__init__()
                            self.features = self._make_layers(cfg[vgg_name])
                            self.classifier = nn.Sequential(
                                nn.Linear(512 * 7 * 7, 512),  # å‡è®¾è¾“å…¥å›¾åƒä¸º 224x224ï¼Œç»è¿‡5æ¬¡æœ€å¤§æ± åŒ–åç‰¹å¾å›¾å°ºå¯¸ä¸º 7x7
                                nn.ReLU(inplace=True),
                                nn.Dropout(0.2),
                                nn.Linear(512, 256),
                                nn.ReLU(inplace=True),
                                nn.Dropout(0.2),  # Dropoutå±‚,å¯ä»¥0.2æ¦‚ç‡ä¸¢å¼ƒä¸€äº›ç¥ç»å…ƒ
                                nn.Linear(256, num_classes)  # å¤šåˆ†ç±»ä»»åŠ¡
                            )

                        def forward(self, x):
                            out = self.features(x)  # é€šè¿‡ç‰¹å¾å±‚æå–
                            out = out.view(out.size(0), -1)
                            out = self.classifier(out)
                            out = F.log_softmax(out, dim=1)  # æ·»åŠ softmaxæ¿€æ´»å‡½æ•°
                            return out

                        # æ ¹æ®é…ç½®åˆ›å»ºç½‘ç»œå±‚
                        def _make_layers(self, cfg):
                            layers = []
                            in_channels = 3  # è¾“å…¥çš„é€šé“æ•°
                            for x in cfg:
                                if x == 'M':  # è¯´æ˜é‡åˆ°äº†æœ€å¤§æ± åŒ–å±‚
                                    layers += [nn.MaxPool2d(kernel_size=2, stride=2, padding=0)]
                                else:
                                    layers += [nn.Conv2d(in_channels, x, kernel_size=3, stride=1, padding=1),
                                               nn.BatchNorm2d(x),
                                               nn.ReLU()
                                               ]
                                    in_channels = x
                            return nn.Sequential(*layers)


                    # æ¨¡å‹é…ç½®
                    cfg = {
                        'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],
                        'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],
                        'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],
                        'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512,
                                  512, 512, 'M'],
                    }

                    # åŠ è½½æ¨¡å‹
                    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

                    # å®šä¹‰ç±»åˆ«
                    classes = ('Harmful', 'Kitchen', 'Other', 'Recyclable')


                    # å®šä¹‰é¢„æµ‹å‡½æ•°
                    def predict_image(image_path, model, classes, device):
                        # åŠ è½½å’Œé¢„å¤„ç†å›¾åƒ
                        image = Image.open(image_path).convert('RGB')
                        transform = transforms.Compose([
                            transforms.Resize((224, 224)),
                            transforms.ToTensor(),
                            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
                        ])
                        img = transform(image)
                        img = img.unsqueeze(0).to(device)

                        # è¿›è¡Œé¢„æµ‹
                        model.eval()
                        with torch.no_grad():
                            output = model(img)
                        prob = F.softmax(output, dim=1)
                        value, predicted = torch.max(output.data, 1)

                        # è¿”å›é¢„æµ‹ç»“æœ
                        return prob, predicted.item(), classes[predicted.item()]


                    # åˆ›å»ºæ¨¡å‹å®ä¾‹
                    model11 = VGG(vgg_name='VGG11', num_classes=4).to(device)
                    model13 = VGG(vgg_name='VGG13', num_classes=4).to(device)
                    model16 = VGG(vgg_name='VGG16', num_classes=4).to(device)
                    model19 = VGG(vgg_name='VGG19', num_classes=4).to(device)

                    # åŠ è½½æ¨¡å‹çš„æƒé‡
                    state_dict11 = torch.load('./model/VGG11.pth', map_location=device)
                    model11.load_state_dict(state_dict11)
                    state_dict13 = torch.load('./model/VGG13.pth', map_location=device)
                    model13.load_state_dict(state_dict13)
                    state_dict16 = torch.load('./model/VGG16.pth', map_location=device)
                    model16.load_state_dict(state_dict16)
                    state_dict19 = torch.load('./model/VGG19.pth', map_location=device)
                    model19.load_state_dict(state_dict19)

                    # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
                    model11.eval()
                    model13.eval()
                    model16.eval()
                    model19.eval()

                    # æ–‡ä»¶ä¸Šä¼ å™¨

                    file_uploader = st.file_uploader("ä¸Šä¼ å›¾ç‰‡ (png/jpg æ ¼å¼)", type=["jpg", "png"])

                    if file_uploader is not None:
                        # é€‰æ‹©æ¨¡å‹
                        model_choice = st.selectbox("é€‰æ‹©æ¨¡å‹", ['VGG11', 'VGG13', 'VGG16', 'VGG19'])

                        # é€‰æ‹©å¯¹åº”çš„æ¨¡å‹
                        if model_choice == 'VGG11':
                            model = model11
                        elif model_choice == 'VGG13':
                            model = model13
                        elif model_choice == 'VGG16':
                            model = model16
                        else:
                            model = model19

                        # é¢„æµ‹å›¾åƒ
                        prob, predicted_index, pred_class = predict_image(file_uploader, model, classes, device)

                        # å±•ç¤ºé¢„æµ‹ç»“æœ
                        st.image(file_uploader, caption='æ˜¾ç¤ºå›¾ç‰‡', use_container_width=True)  # å±•ç¤ºä¸Šä¼ çš„å›¾åƒ
                        st.write(f"æ¨¡å‹{model_choice}çš„é¢„æµ‹:")
                        for i, p in enumerate(prob[0], start=0):  # å‡è®¾ prob[0] æ˜¯ä¸€ä¸ªåŒ…å«æ‰€æœ‰æ¦‚ç‡çš„å¼ é‡
                            st.write(f'é¢„æµ‹ä¸º{classes[i]}åƒåœ¾æ¦‚ç‡ä¸º: {p:.4f}')
                        st.write(f"æœ€ç»ˆé¢„æµ‹ç±»åˆ«ä¸º: {pred_class}")
                    else:
                        st.write("Please upload an image to see the prediction.")

    if selected == 'æ„è§åé¦ˆ':
        def get_image_base64(image_path):
            with open('./picture/åé¦ˆ.gif', "rb") as img_file:
                encoded = base64.b64encode(img_file.read()).decode("utf-8")
            return encoded
        st.markdown("<h2 style='text-align: center; color: #4CAF50;'>æ„è§åé¦ˆ</h2>", unsafe_allow_html=True)
        # ä½¿ç”¨è¡¨å•
        with st.form('my_form'):
            name = st.text_input(label='è¯·è¾“å…¥ä½ çš„åå­— ğŸ‘¤')
            email = st.text_input(label='è¯·è¾“å…¥ä½ çš„é‚®ç®± ğŸ“§')
            content = st.text_area(label='è¯·è¾“å…¥ä½ çš„åé¦ˆæ„è§ âœï¸')

            submit = st.form_submit_button(
                'æäº¤',
                type="primary"  # è®¾ç½®æŒ‰é’®é£æ ¼
            )
            if submit:
                # GIF å›¾ç‰‡è·¯å¾„
                gif_path = r"C:\Users\ææ³“è‹\Desktop\å°ç»„ä½œä¸š\picture\åé¦ˆ.gif"

                try:
                    gif_base64 = get_image_base64(gif_path)
                    st.markdown(
                        f"""
                            <div style="text-align: center;">
                                <img src="data:image/gif;base64,{gif_base64}" alt="åé¦ˆæˆåŠŸ" width="400" style="border-radius: 10px;">
                                <h3 style="color: #4CAF50;">æ„Ÿè°¢æ‚¨çš„åé¦ˆ! ğŸ™</h3>
                            </div>
                            """,
                        unsafe_allow_html=True
                    )
                except FileNotFoundError:
                    st.error("GIF å›¾ç‰‡æœªæ‰¾åˆ°ï¼Œè¯·æ£€æŸ¥è·¯å¾„ï¼")